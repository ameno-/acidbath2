# SUMMARY

Developer demonstrates how MCP servers like Serena and Refactor MCP can dramatically improve AI coding assistant performance on legacy codebases.

# IDEAS

- AI coding agents often fail on legacy codebases due to improper tool usage rather than fundamental limitations.
- Using AI coding tools without proper setup is like eating spaghetti with a spoon instead of fork.
- Serena provides semantic search and edit capabilities that outperform basic text-based brute force approaches significantly.
- Semantic code retrieval tools reduce token consumption and improve AI assistant performance on large codebases dramatically.
- MCP servers can be installed locally and run for free without requiring expensive cloud-based solutions or subscriptions.
- Refactoring tasks that take humans 12 minutes can be completed by AI assistants in 6 minutes effectively.
- Legacy codebases with 842,000 lines of code can overwhelm standard AI coding assistants without proper tooling.
- Brute force AI approaches can take 3 hours and cost $14 for simple renaming tasks unnecessarily.
- MCP servers reduce token usage by 28x and costs by 20x while improving speed by 30x significantly.
- Proper tooling eliminates doom looping where AI agents repeatedly fail, compile, retry in endless frustrating cycles.
- Semantic search tools work better on large codebases than small ones due to complexity scaling advantages.
- Factory method patterns and test-driven development can be implemented automatically by properly configured AI assistants.
- Unit test verification should always be included as part of AI-assisted refactoring workflows for quality assurance.
- Roslyn-based refactoring tools provide language-specific intelligent code manipulation capabilities for .NET development environments specifically.
- Autonomous refactoring becomes possible with proper MCP server configuration, requiring minimal human intervention during complex operations.
- IDE refactoring capabilities combined with AI assistance create powerful workflows for large-scale code transformations efficiently.
- Token budget optimization through proper tooling allows developers to work longer and faster with AI assistants.
- Multi-step refactoring processes become hands-off operations when AI assistants have access to intelligent semantic tools.
- Open source MCP servers provide free alternatives to expensive commercial AI coding solutions and subscriptions.
- Embedding-based searches outperform traditional text searches for code navigation in large legacy codebases significantly.
- Build and compile cycles consume most time in AI coding sessions when proper tooling isn't available.
- Code quality obsession can be reduced when AI assistants have proper tools to handle refactoring automatically.
- Weekly AI token budgets stretch much further when using semantic search and intelligent editing tools properly.
- Meeting attendance becomes possible during AI refactoring sessions when tools enable autonomous operation without constant supervision.
- Colleague coaching time increases when developers aren't manually babysitting AI coding assistant doom loops constantly.

# INSIGHTS

- Proper tooling transforms AI coding assistants from expensive, slow helpers into autonomous, efficient development partners.
- Token efficiency matters more than raw AI capability when working with large legacy codebases regularly.
- Semantic understanding beats brute force text processing for code navigation and editing in complex systems.
- MCP servers bridge the gap between AI language models and specialized development tools effectively.
- Time investment in tool setup pays massive dividends in reduced supervision and increased autonomous operation.
- Legacy codebase challenges stem from tooling limitations rather than fundamental AI assistant capability deficiencies.
- Cost optimization through intelligent tooling makes AI coding assistance sustainable for regular development work.
- Autonomous refactoring becomes reality when AI assistants have access to language-specific intelligent editing capabilities.
- Developer productivity multiplies when AI assistants can work independently without constant human intervention and guidance.
- Open source solutions often provide better value than expensive commercial AI coding platforms and services.

# QUOTES

- "these AI coding agents, they don't work for my context. My codebase is legacy. It's too big."
- "I like to compare that to eating spaghetti with a spoon. It's technically possible, but it'll go a lot faster if you use a damn fork."
- "I want to show you how to 10x, 20x, or even 30x performance of your AI coding assistants without even having to resort to sub agents."
- "It's a smarter way to uh search for code or search through code bases and to uh edit code."
- "This is a really mid small codebase, this ehop on web thing. So it's not really uh that impressive."
- "Your uh weekly anthropic token budget will uh be way more. You you will be able to run a lot faster and a lot longer."
- "That's twice as fast as I would be able to do this."
- "I can go do something else, attend a meeting, coach a colleague, whatever."
- "This is exactly uh the way I would have done it."
- "It has to do less build, fail, compile, retry um loops and the token spend is impressively lower."
- "There's 55,000 files. And there are 842,000 TypeScript files and 360,000 lines of C."
- "This just took way too long to capture on video. So it got there in the end."
- "It had to rebuild the code four times and it took three whole hours."
- "It provides a server that exposes tools to do refactorings for you."
- "143 files were changed. That sounds correct."
- "This was a oneshot uh prompt. So I I just put prompt and it went on and did its thing."
- "It's not as fast as doing it yourself, but it's like autonomous, which is pretty interesting."
- "That is a factor of roughly 28 times less token spent and a factor of 20 times uh cheaper."
- "That's also like a 30x improvement."
- "Please before using sub agents take a look at this stuff."

# HABITS

- Always run unit tests after completing any refactoring task to verify code still functions correctly.
- Use semantic search tools instead of brute force text searches when working with large codebases.
- Install MCP servers locally to avoid paying for expensive cloud-based AI coding solutions unnecessarily.
- Include build verification as part of every AI-assisted refactoring workflow to catch compilation errors early.
- Start with proper tool setup before attempting complex refactoring tasks with AI coding assistants regularly.
- Verify AI assistant plans before execution to avoid wasting time on incorrect approaches or strategies.
- Use language-specific refactoring tools rather than generic text manipulation for better results and efficiency.
- Measure token usage and costs to optimize AI assistant workflows for budget efficiency and sustainability.
- Configure AI assistants with specialized tools before starting work on legacy codebases for better performance.
- Include timer tracking during AI coding sessions to measure and compare performance improvements objectively.
- Prioritize autonomous operation over manual intervention when configuring AI coding assistant workflows for efficiency.
- Use open source MCP servers when available to reduce costs and maintain control over development tools.
- Test refactoring approaches on smaller codebases first before applying to large legacy systems for validation.
- Document successful MCP server configurations for reuse across different projects and development environments consistently.
- Share effective tool combinations with team members to improve overall development productivity and code quality.

# FACTS

- Umbraco content management system contains 55,000 files with 842,000 lines of TypeScript and C# code.
- Manual refactoring of heavily used types in large codebases typically takes approximately 3 minutes including compilation.
- Brute force AI coding approaches can consume 28 million tokens costing $14 for simple renaming tasks.
- MCP servers can reduce token usage by 28x and costs by 20x while improving speed 30x.
- Serena MCP server provides semantic search and edit capabilities for improved code navigation and manipulation.
- Refactor MCP server offers Roslyn-based refactoring tools specifically designed for .NET development environments and workflows.
- AI coding assistants without proper tooling can take 3 hours for tasks completable in minutes.
- Semantic search tools perform better on large codebases than small ones due to complexity advantages.
- Open source MCP servers can run locally without requiring expensive cloud subscriptions or commercial licensing.
- Factory method implementation and test-driven development can be automated through properly configured AI assistants.
- Legacy codebase challenges often stem from improper tool usage rather than fundamental AI limitations or capabilities.
- Unit test suites in large codebases typically require 30 seconds to complete full execution cycles.
- IDE refactoring tools like JetBrains Rider provide built-in support for multi-file code transformations efficiently.
- Token budget optimization allows developers to work longer sessions with AI assistants without exceeding spending limits.
- Doom looping occurs when AI assistants repeatedly fail compilation cycles without intelligent editing tools available.

# REFERENCES

- Serena MCP server for semantic search and edit capabilities
- Refactor MCP server by Dave Hillier for .NET refactoring
- Umbraco content management system as large codebase example
- GitHub repositories for open source MCP server implementations
- JetBrains Rider IDE for refactoring support and capabilities
- Anthropic Claude Code AI coding assistant platform
- Roslyn analyzers for C# code analysis and refactoring
- Clock count lines of code tool for codebase analysis
- MCP (Model Context Protocol) server architecture and documentation
- .NET development environment and build tools integration

# ONE-SENTENCE TAKEAWAY

Proper MCP server tooling transforms AI coding assistants into autonomous, efficient partners for legacy development.

# RECOMMENDATIONS

- Install semantic search MCP servers before attempting AI-assisted refactoring on large legacy codebases for efficiency.
- Use language-specific refactoring tools rather than generic text manipulation for better AI coding assistant performance.
- Measure token usage and costs to optimize AI assistant workflows and avoid expensive doom loops.
- Configure AI assistants with proper tools before starting work to achieve 20-30x performance improvements consistently.
- Always include unit test verification in AI-assisted refactoring workflows to maintain code quality and functionality.
- Choose open source MCP servers when available to reduce costs and maintain development tool control.
- Set up autonomous operation workflows to enable meeting attendance during AI refactoring sessions for productivity.
- Use timer tracking during AI coding sessions to measure and compare performance improvements objectively.
- Prioritize tool setup investment over expensive commercial AI coding platforms for better long-term value.
- Share successful MCP server configurations with team members to improve overall development productivity significantly.
- Test AI assistant approaches on smaller codebases first before applying to large legacy systems.
- Include build verification as part of every AI-assisted refactoring workflow to catch errors early.
- Document effective tool combinations for reuse across different projects and development environments for consistency.
- Use embedding-based searches instead of traditional text searches for better code navigation in large codebases.
- Configure weekly AI token budgets to stretch further with proper semantic search and editing tools.
