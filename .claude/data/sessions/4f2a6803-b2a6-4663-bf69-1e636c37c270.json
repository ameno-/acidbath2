{
  "session_id": "4f2a6803-b2a6-4663-bf69-1e636c37c270",
  "prompts": [
    "/commit sdlc_planner feature {\"number\": \"content-extractors\", \"title\": \"Content Extractors - Multi-Type Content Extraction Pipeline\", \"body\": \"## Summary\\n\\nCreate content extraction modules that can process multiple input types (YouTube, URL, PDF, text) and produce a unified content object for pattern analysis.\\n\\n## Parent Issue\\n\\nThis is Phase 3 of `issue-jerry-patterns-content-analysis.md`\\n\\n## Dependencies\\n\\n- None (can run in parallel with Phase 2)\\n\\n## Deliverables\\n\\n### 1. Content Types Enum and Data Classes\\n\\n```python\\nfrom enum import Enum\\nfrom dataclasses import dataclass\\nfrom typing import Optional\\n\\nclass ContentType(Enum):\\n    YOUTUBE = \\\"youtube\\\"\\n    URL = \\\"url\\\"\\n    PDF = \\\"pdf\\\"\\n    TEXT = \\\"text\\\"\\n\\n@dataclass\\nclass ContentObject:\\n    type: ContentType\\n    text: str\\n    source: str  # Original input (URL, file path, etc.)\\n    metadata: dict  # Type-specific metadata\\n\\n    # YouTube-specific\\n    video_id: Optional[str] = None\\n    transcript_path: Optional[str] = None\\n\\n    # URL-specific\\n    url: Optional[str] = None\\n    title: Optional[str] = None\\n\\n    # File-specific\\n    file_path: Optional[str] = None\\n    file_size: Optional[int] = None\\n```\\n\\n### 2. Content Type Detection\\n\\n```python\\ndef detect_content_type(input_source: str) -> ContentType:\\n    \\\"\\\"\\\"\\n    Detect content type from input source.\\n\\n    Rules:\\n    - youtube.com or youtu.be in URL \\u2192 YOUTUBE\\n    - http:// or https:// \\u2192 URL\\n    - .pdf extension or PDF magic bytes \\u2192 PDF\\n    - Everything else \\u2192 TEXT\\n    \\\"\\\"\\\"\\n```\\n\\n### 3. YouTube Extractor (`youtube_ops.py`)\\n\\nWrapper around yt-dlp for transcript and metadata extraction:\\n\\n```python\\ndef extract_youtube_metadata(url: str) -> dict:\\n    \\\"\\\"\\\"Extract video metadata using yt-dlp --dump-json.\\\"\\\"\\\"\\n\\ndef download_transcript(url: str, output_dir: str) -> str:\\n    \\\"\\\"\\\"Download transcript using yt-dlp subtitles.\\\"\\\"\\\"\\n\\ndef extract_youtube_content(url: str, output_dir: str) -> ContentObject:\\n    \\\"\\\"\\\"Full YouTube content extraction pipeline.\\\"\\\"\\\"\\n\\ndef classify_youtube_content(metadata: dict, transcript: str) -> str:\\n    \\\"\\\"\\\"Classify as technical/educational/general.\\\"\\\"\\\"\\n```\\n\\n### 4. URL Extractor (`web_ops.py`)\\n\\nWeb page scraping and content extraction:\\n\\n```python\\ndef scrape_url_to_markdown(url: str) -> str:\\n    \\\"\\\"\\\"Scrape URL and convert to clean markdown.\\\"\\\"\\\"\\n\\ndef extract_url_content(url: str) -> ContentObject:\\n    \\\"\\\"\\\"Full URL content extraction pipeline.\\\"\\\"\\\"\\n```\\n\\nOptions:\\n- Use `fabric --scrape_url` if available\\n- Fall back to basic requests + html2text\\n- Handle common edge cases (paywalls, JS-heavy sites)\\n\\n### 5. PDF Extractor (`pdf_ops.py`)\\n\\nPDF text extraction:\\n\\n```python\\ndef extract_pdf_text(file_path: str) -> str:\\n    \\\"\\\"\\\"Extract text from PDF file.\\\"\\\"\\\"\\n\\ndef extract_pdf_content(file_path: str) -> ContentObject:\\n    \\\"\\\"\\\"Full PDF content extraction pipeline.\\\"\\\"\\\"\\n```\\n\\nOptions:\\n- Use pypdf or pdfplumber\\n- Handle scanned PDFs gracefully (note limitation)\\n\\n### 6. Text Extractor (`text_ops.py`)\\n\\nSimple text handling:\\n\\n```python\\ndef read_text_file(file_path: str) -> str:\\n    \\\"\\\"\\\"Read text from file.\\\"\\\"\\\"\\n\\ndef extract_text_content(source: str) -> ContentObject:\\n    \\\"\\\"\\\"Handle text input (file or direct string).\\\"\\\"\\\"\\n```\\n\\n### 7. Unified Extractor (`content_extractors.py`)\\n\\nMain entry point:\\n\\n```python\\ndef extract_content(input_source: str, content_type: ContentType = None) -> ContentObject:\\n    \\\"\\\"\\\"\\n    Extract content from any supported input type.\\n\\n    Auto-detects type if not specified.\\n    Routes to appropriate extractor.\\n    Returns unified ContentObject.\\n    \\\"\\\"\\\"\\n\\nCONTENT_EXTRACTORS = {\\n    ContentType.YOUTUBE: extract_youtube_content,\\n    ContentType.URL: extract_url_content,\\n    ContentType.PDF: extract_pdf_content,\\n    ContentType.TEXT: extract_text_content,\\n}\\n```\\n\\n## Acceptance Criteria\\n\\n- [ ] `ContentType` enum and `ContentObject` dataclass defined\\n- [ ] `detect_content_type()` correctly identifies all types\\n- [ ] YouTube extractor downloads transcripts via yt-dlp\\n- [ ] URL extractor scrapes web pages to markdown\\n- [ ] PDF extractor extracts text from PDF files\\n- [ ] Text extractor handles files and direct input\\n- [ ] Unified `extract_content()` routes correctly\\n- [ ] All extractors return consistent `ContentObject`\\n\\n## Technical Notes\\n\\n- yt-dlp is a required dependency (must be installed)\\n- Web scraping may require handling rate limits\\n- PDF extraction is best-effort (scanned PDFs won't work)\\n- Text detection includes stdin (`-`) support\\n\\n## Test Cases\\n\\n```python\\n# YouTube\\ncontent = extract_content(\\\"https://youtube.com/watch?v=abc123\\\")\\nassert content.type == ContentType.YOUTUBE\\nassert content.video_id == \\\"abc123\\\"\\n\\n# URL\\ncontent = extract_content(\\\"https://example.com/article\\\")\\nassert content.type == ContentType.URL\\nassert len(content.text) > 0\\n\\n# PDF\\ncontent = extract_content(\\\"/path/to/document.pdf\\\")\\nassert content.type == ContentType.PDF\\n\\n# Text\\ncontent = extract_content(\\\"/path/to/notes.txt\\\")\\nassert content.type == ContentType.TEXT\\n```\\n\\n## Files to Create/Modify\\n\\n**Create:**\\n- `adws/adw_modules/content_extractors.py` (main module)\\n- `adws/adw_modules/youtube_ops.py` (YouTube-specific)\\n- `adws/adw_modules/web_ops.py` (URL scraping)\\n- `adws/adw_modules/pdf_ops.py` (PDF extraction)\\n- `adws/adw_modules/text_ops.py` (text handling)\\n\\n**Modify:**\\n- `adws/adw_modules/__init__.py` (export new modules)\"}"
  ]
}